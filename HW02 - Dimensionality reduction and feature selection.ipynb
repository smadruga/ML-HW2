{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning\n",
    "\n",
    "# Practical Work 2: Seleção de Características e Redução da Dimensionalidade \n",
    "\n",
    "### Alunos:\n",
    "- Adriel Santos\n",
    "- Rodrigo Campo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução \n",
    " \n",
    "Ao se trabalhar com atividades de aprendizado de máquina, citando aqui classificaçao e regressão como exemplos, inevitavelmente fazemos uso de bases de dados que correspondem ao contexto do trabalho. Estes dados podem ser extraidos de diversas fontes como internet, imagens, sensores e até mesmo de banco de dados, dando origem, muitas vezes, à uma base composta por diversos atributos que, em conjunto, podem rotular um objeto.\n",
    "\n",
    "É sabido que para gerar um bom modelo de predição é necessário ter dados suficientemente bons, capazes de determinar os rótulos dos objetos de uma base. Ao contrário do que o senso comum nos diz, nem sempre mais é melhor quando estamos falando de classificação ou regressão. O excesso de atributos pode, muitas vezes, por a perder o processo de predição por ter dados não coerentes com o contexto trabalhado. Frequentemente torna-se necessário então, limitar a quantidade de atributos de entrada para se obter um bom modelo de predição. Além disso, um menor número de atributos pode melhorar o desempenho do processo de geração do modelo. Quando o espaço de características contém somente as características mais salientes, o classificador será mais rápido e ocupará menos memória [Jain et al., 2000].\n",
    "\n",
    "Diante disso, o objetivo deste trabalho é fazer uma introdução sobre técnicas de seleção de características, também conhecida na literatura como seleção de atributos. Além disso, o trabalho aponta diferenças, de acordo com alguns autores, entre seleção de características e redução da dimensionalidade. Neste contexto, o trabalho trás ainda alguns exemplos práticos para melhor entendimento das técnicas apresentadas. \n",
    "\n",
    "\n",
    "## Seleção de Características e Redução da Dimensionalidade: existe diferenças?\n",
    " \n",
    "Ambas metodologias buscam reduzir o número de características/atributos selecionado o menor conjunto que pode resultar em um  desempenho de predição satisfatório. O Objetivo é sempre obter as características mais relevantes ao modelo de predição. Na liteturatura são encontrados trabalhos que consideram as duas técnicas como sinônimas e ainda autores que apontam diferenças, sutís ou não, entre elas.\n",
    "\n",
    "Partindo do conceito que apresenta dimensionalidade, neste contexto, como o número de características necessárias para representar um padrão, podemos considerar que os termos são sinômimos. Contudo há autores que delimitam o significado das técnicas considerando a forma como os dados são gerados. Fulano (Ano - FONTE ONDE VC PEGOU AS INFO RODRIGO) , e Campos (2001) nos apresentam:\n",
    "\n",
    "- Redução dao da dimensionalidade: Etapa de criação de novas atributos a partir de um grupo de características originais. Por exemplo: reduzir a dimensionalidade de uma imagem para melhorar o processo de predição de um modelo, uma vez que, originalmente, uma imagem apresenta dimensionalidade M, onde M = altura X comprimento X precisão do pixel. Este \"excesso\" de dados provavelmente provocaria uma desestabilização no modelo.\n",
    "\n",
    "- Seleção de Características: Processo que obtém um subgrupo de características, consideradas como as mais relevantes, a partir do conunto original de características. Por exemplo, escolher em uma base com 80 atributos, quais são os 30 que compoêm o conjunto que mais favorece o modelo de predição. \n",
    "\n",
    "As duas técnicas podedm ainda ser utilizadas em conjunto. Situações mais rotineiras do uso híbrido das duas técnicas são encontradas em trabalhos que envolvem imagens digitais. Resmini (2016), por exemplo, combina as duas tecnicas extraindo diversos dados de imagens digitais da mama e selecionando posteriormente quais são os atributos que melhor contribuem para o modelo de classificação.\n",
    "\n",
    "Este trabalho apresenta técnicas de  de redução da dimensionalidade usando imagens digitais e extraindo dados, a partir de descritores estatísticos. Em contra ponto, apresenta também técnicas de seleção de características a partir de um conjunto de dados originais já formados. \n",
    "\n",
    "##  Redução da dimensionalidade\n",
    "\n",
    "Inserir algorítmos de extração de características de imagens digitais.\n",
    "Dar exemplos e plotar gráficos\n",
    " \n",
    "## Feature Selection\n",
    " \n",
    "É o processo de seleção automática dos atributos mais relevantes para o modelo de predição a ser trabalhado, incluindo ou excluindo atributos presentes no conjunto de dados sem alterá-los.\n",
    " \n",
    "Este método de seleção é bastante útil na hora de criar um modelo preditivo de melhor qualidade, escolhendo os recursos que oferecem melhor precisão, exigindo menos dados ao identificar e remover atributos irrelevantes ou redundantes (que não contribuem ou podem influenciar negativamente na precisão do modelo).\n",
    " \n",
    "Trabalhar com menos atributos é mais interessante pois diminui a complexidade do modelo, tornando-o menos genérico, mais simples de entender e explicar e reduz o tempo de treinamento. Portanto, o objetivo do feature selection pode ser definido por 3 pontos principais:\n",
    " \n",
    "melhorar o desempenho da predição do modelo;\n",
    "oferecer preditores mais rápidos e otimizados;\n",
    "garantir que o processamento dos dados seja claro e de fácil compreensão.\n",
    " \n",
    "Portanto, o feature selection é um passo importante do processo de construção do modelo. Ignorar esta etapa pode introduzir uma tendência negativa no modelo ou torná-lo genérico demais, além de diminuir a acurácia em diversos modelos, especialmente algoritmos lineares tais como: linear e logistic regression.\n",
    " \n",
    "Ao selecionar os atributos mais úteis para um modelo de predição vale a pena considerar os seguintes pontos:\n",
    " \n",
    "Avaliar a área de conhecimento com a qual se está trabalhando. Havendo domínio sobre ela então a sugestão é buscar um grupo de atributos que permita tirar o máximo proveito dos dados;\n",
    "Verificar a compatibilidade entre os atributos e, quando não houver, normalizá-los\n",
    "Ao observar interdependência entre os atributos é interessante expandir os recursos através da construção de recursos conjuntivos, dentro das capacidades do computador;\n",
    "Gerar atributos disjuntivos ou somas ponderadas em casos de não necessidade de corte de variáveis de entrada;\n",
    "Utilizar um método de classificação variável para avaliar atributos individualmente e entender sua influência no sistema. Este método irá ajudar a identificar dados sujos/irrelevantes que poderão ser descartados.\n",
    "Comparar diferentes métodos de seleção de atributos e avaliar qual a melhor abordagem de acordo com o modelo de predição em questão\n",
    " \n",
    "### Algoritmos de seleção de atributos\n",
    " \n",
    "Existem três classes de algoritmos de feature selection: filter methods, wrapper methods e embedded methods.\n",
    " \n",
    "* Filter Methods\n",
    " \n",
    "A seleção de recursos através de métodos de filtragem utiliza-se de medidas estatísticas para atribuir um valor para cada atributo. Então, uma lista ordenada é gerada, que servirá de base para decidir qual característica será mantida ou removida do modelo. \n",
    " \n",
    "Este tipo de método geralmente considera uma única variável, analisando cada atributo de maneira independente ou de acordo com sua variável dependente.\n",
    " \n",
    "Alguns exemplos de algoritmos deste tipo são: Chi squared test, information gain e correlation coefficient scores.\n",
    " \n",
    "* Wrapper Methods\n",
    " \n",
    "Trata a seleção de atributos como um problema de busca, onde diferentes combinações são preparadas, avaliadas e comparadas com entre si. Um modelo preditivo é utilizado para avaliar a combinação de recursos e atribuir uma nota de acordo com a precisão do modelo.\n",
    " \n",
    "O processo de busca pode ser metódico (ex: busca do tipo best-first), estocástico (ex: algoritmo random hill-climbing), ou utilizar heurísticas (ex: forward/backward passes) com o intuito de adicionar ou remover atributos. \n",
    " \n",
    "Um exemplo de algoritmo desta classe é o recursive feature elimination.\n",
    " \n",
    "* Embedded Methods\n",
    " \n",
    "Aprendem qual características oferecem melhor contribuição para a acurácia do modelo enquanto o modelo é criado. Os métodos de regularização (regularization methods) são os tipos mais comum deste método, também conhecidos como métodos de penalização. \n",
    " \n",
    "Introduzem uma restrição adicional na otimização do algoritmo preditivo, levando-o a um modelo de menor complexidade (menos coeficientes).\n",
    " \n",
    "Exemplos de algoritmos desta classe são: LASSO, Elastic Net e Ridge Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div class=\"container-fluid\">\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col-md-4\" align='center'></div>\n",
    "        <div class=\"col-md-4\" align='center'>\n",
    "            <img src='http://machinelearningmastery.com/wp-content/uploads/2014/03/feature-selection.jpg'/>\n",
    "            Feature Selection\n",
    "        </div>\n",
    "        <div class=\"col-md-6\" align='center'></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemplos de Feature Selection usando Python\n",
    " \n",
    "A seguir serão apresentados 4 algoritmos de feature selection, aplicados em machine learning, utilizando Python.\n",
    " \n",
    "Para esses exemplos utiliza-se como base de dados o Pima Indians onset of diabetes dataset, problema de classificação binário onde todo os atributos são numéricos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Univariate Selection\n",
    " \n",
    "Testes estatísticos podem ser utilizados para selecionar os atributos com um vínculo mais forte com a variável de saída. A biblioteca scikit-learn oferece uma classe chamada SelectKBest que pode ser usada com um conjunto de diferentes testes estatísticos com o propósito de escolher um número específico de características.\n",
    " \n",
    "O exemplo a seguir usa o teste estatístico chi quadrado (chi²) para atributos não negativos, onde seleciona os quatro melhores recursos da base de dados em questão.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "resumo do placar:\n",
      " [  111.52   1411.887    17.605    53.108  2175.565   127.669     5.393\n",
      "   181.304]\n",
      "\n",
      "resumo dos atributos selecionados:\n",
      " [[ 148.     0.    33.6   50. ]\n",
      " [  85.     0.    26.6   31. ]\n",
      " [ 183.     0.    23.3   32. ]\n",
      " [  89.    94.    28.1   21. ]\n",
      " [ 137.   168.    43.1   33. ]]\n"
     ]
    }
   ],
   "source": [
    "# (Chi-quadrado para classificação)\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "# carregamento dos dados\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = pandas.read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# extração dos atributos\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# resumo do placar\n",
    "numpy.set_printoptions(precision=3)\n",
    "print(\"\\nresumo do placar:\\n\", fit.scores_)\n",
    "\n",
    "features = fit.transform(X)\n",
    "\n",
    "# resumo dos atributos selecionados\n",
    "print(\"\\nresumo dos atributos selecionados:\\n\", features[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao executar o algoritmo acima é possível observar os valores atribuídos a cada atributo de acordo com sua relevância, formando um ranking e destacando os quatro melhores: plas, test, mass e age"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recursive Feature Elimination (RFE)\n",
    " \n",
    "Método que trabalha removendo atributos recursivamente e monta o modelo com aqueles remanescentes. Identifica quais características do dataset (e possíveis combinações) contribuem mais para a predição do atributo alvo através do modelo de acurácia.\n",
    " \n",
    "No exemplo a seguir usa-se o RFE com o algoritmo de regressão logística para selecionar os 3 melhores atributos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de atributos: 3\n",
      "Atributos selecionados: [ True False False False False  True  True False]\n",
      "Ranking: [1 2 3 5 6 1 1 4]\n"
     ]
    }
   ],
   "source": [
    "# Seleção de atributos com RFE\n",
    "from pandas import read_csv\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# carregamento dos dados\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# extração dos atributos\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, 3)\n",
    "fit = rfe.fit(X, Y)\n",
    "\n",
    "print((\"Quantidade de atributos: %d\") % fit.n_features_)\n",
    "print((\"Atributos selecionados: %s\") % fit.support_)\n",
    "print((\"Ranking: %s\") % fit.ranking_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No resultado acima, gerado pelo RFE, são apresentado os 3 melhores atributos: preg, mass e pedi, marcados como TRUE no array support\\_ e sinalizados como “1” no array ranking\\_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA)\n",
    "A Análise dos Componentes Principais usa álgebra linear para transformar um banco de dados em uma forma compacta. De maneira geral esta é conhecida como uma técnica de redução de dados. \n",
    " \n",
    "Uma propriedade do PCA é a escolha do número de dimensões ou o componente principal do resultado transformado.\n",
    " \n",
    "No exemplo abaixo utiliza-se o PCA e seleciona-se três características principais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variação: [ 0.889  0.062  0.026]\n",
      "[[ -2.022e-03   9.781e-02   1.609e-02   6.076e-02   9.931e-01   1.401e-02\n",
      "    5.372e-04  -3.565e-03]\n",
      " [ -2.265e-02  -9.722e-01  -1.419e-01   5.786e-02   9.463e-02  -4.697e-02\n",
      "   -8.168e-04  -1.402e-01]\n",
      " [ -2.246e-02   1.434e-01  -9.225e-01  -3.070e-01   2.098e-02  -1.324e-01\n",
      "   -6.400e-04  -1.255e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Seleção de atributos com PCA\n",
    "import numpy\n",
    "\n",
    "from pandas import read_csv\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# carregamento dos dados\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# seleção dos atributos\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)\n",
    "\n",
    "# resumo\n",
    "print((\"Variação: %s\") % fit.explained_variance_ratio_)\n",
    "print((fit.components_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como pode ser visto no resultado o banco de dados transformado (com os 3 principais componentes) tem pouca semelhança com o banco de dados original."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Importance\n",
    " \n",
    "Métodos de árvores de decisão combinadas, como Random Forest e Extra Trees, podem ser utilizadas para medir a relevância dos atributos.\n",
    " \n",
    "No exemplo abaixo construiu-se um classificador ExtraTreesClassifier para o dataset em questão. No resultado gerado pode-se notar o grau de importância de cada atributo. Quanto maior o valor mais importante ele é. No resultado gerado os mais importantes são plas, age e mass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.108  0.217  0.104  0.087  0.073  0.138  0.125  0.148]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance with Extra Trees Classifier\n",
    "from pandas import read_csv\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "# carregamento dos dados\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\"\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "\n",
    "dataframe = read_csv(url, names=names)\n",
    "array = dataframe.values\n",
    "\n",
    "X = array[:,0:8]\n",
    "Y = array[:,8]\n",
    "\n",
    "# seleção dos atributos\n",
    "model = ExtraTreesClassifier()\n",
    "model.fit(X, Y)\n",
    "\n",
    "print(model.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Referências\n",
    "\n",
    "\n",
    "CAMPOS, Teófilo Emıdio de. Técnicas de seleçao de caracterısticas com aplicaçoes em reconhecimento de faces. 2001. Tese de Doutorado. Master’s thesis, Universidade de Sa Paulo.\n",
    "\n",
    "JAIN, Anil K.; DUIN, Robert P.. W.. ; MAO, Jianchang. Statistical pattern recognition: A review. IEEE Transactions on pattern analysis and machine intelligence, v. 22, n. 1, p. 4-37, 2000.\n",
    "APA\t\n",
    "\n",
    "RESMINI, Roger. Classificação de doenças da mama usando imagens por infravermelho. 2016. Tese de Doutorado, Universidade Federal Fluminense\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
